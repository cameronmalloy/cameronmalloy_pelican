<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Cameron Malloy</title><link href="http://localhost:8080/" rel="alternate"></link><link href="http://localhost:8080/feeds/all.atom.xml" rel="self"></link><id>http://localhost:8080/</id><updated>2020-06-08T15:30:00-07:00</updated><entry><title>Have NFL Rules Affected Concussion Rates?</title><link href="http://localhost:8080/concussions.html" rel="alternate"></link><published>2020-06-08T15:30:00-07:00</published><updated>2020-06-08T15:30:00-07:00</updated><author><name>Cameron Malloy</name></author><id>tag:localhost,2020-06-08:/concussions.html</id><summary type="html">&lt;p&gt;Have the new NFL rules significantly reduced the number of concussions in a season? Or has the decrease just been random chance?&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Have the new NFL rules significantly reduced the number of concussions in a season? Or has the decrease just been random chance?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;After the discovery of &lt;a href="https://en.wikipedia.org/wiki/Chronic_traumatic_encephalopathy" target="_blank"&gt;CTE&lt;/a&gt; by &lt;a href="https://en.wikipedia.org/wiki/Bennet_Omalu" target="_blank"&gt;Dr. Bennet Omalu&lt;/a&gt;, the NFL has been under scrutiny for dismissing the effects of concussions and it's relationship to CTE. Over the past couple of years, the league has started to make rules to help defend players in an attempt to decrease the number of concussions. In 2017 the implemented the &lt;a href="https://en.wikipedia.org/wiki/Helmet-to-helmet_collision#:~:text=In%20the%20NFL%2C%20helmet%2Dto,of%2015%20yards%20for%20violations.&amp;amp;text=In%202017%2C%20the%20NFL%20adopted,offenders%20out%20from%20the%20game." target="_blank"&gt;targeting&lt;/a&gt; rule which fines and throws players out of the game for purposeful helmet-to-helmet collisions. After that they changed the &lt;a href="https://www.sbnation.com/2018/5/22/17369774/nfl-kickoff-rule-change-explained" target="_blank"&gt;rules for kick offs&lt;/a&gt; to make them safer, there was a pretty big decrease in the number of concussions in 2018. There have been some &lt;a _target="_blank&amp;quot;" href="https://www.nfl.com/news/nfl-sees-significant-drop-in-concussions-during-2018-season-0ap3000001013041"&gt;articles&lt;/a&gt; touting this progress, but is this deserved? After a slight increase in 2019, it's not clear cut as to whether these changes had any effect at all. We'll discuss how effective these rules have been and how well the NFL has done with these rules.&lt;/p&gt;
&lt;p&gt;I first learned about these rules, especially the kickoff rules from &lt;a href="https://www.youtube.com/watch?v=4W9WsYPEMHM" target="_blank"&gt;Pat McAfee's video about NFL rule changes in 2020&lt;/a&gt;. It's a great video to see a former player's perspective to these rule changes.&lt;/p&gt;
&lt;p&gt;There have been rules put in place before 2017, however, the NFl only gives us data from 2012. Data before then may not be so great because concussion protocols and documenting concussions were more relaxed before then, so this is all the data I'm comfortable with analyzing.&lt;/p&gt;
&lt;p&gt;Download the &lt;a href="http://localhost:8080/assets/concussions_assets/tests.Rmd"&gt;R-Notebook for the statistical tests in this blog posts. There are only code blocks for permutation and multivariate regression tests, no explanations/comments&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Download the &lt;a href=./assets/concussions_assets/get_supplement_data.ipynb&gt;Python Jupyter notebook to obtain supplemental data (see data section) for multivariate regression tests&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The NFL only has concussion data dating back to 2012 which can be found at &lt;a href="https://www.playsmartplaysafe.com/newsroom/reports/injury-data/" target="_blank"&gt;playsmartplaysafe.com&lt;/a&gt;. We'll only be working with the regular season totals shown below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img not found" src="http://localhost:8080/assets/concussions_assets/concussions_table.png"&gt;
&lt;figcaption markdown="span"&gt;
Screenshot from &lt;a href="https://www.playsmartplaysafe.com/newsroom/reports/injury-data/" target="_blank"&gt;playsmartplaysafe in 2020&lt;/a&gt;
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;Later in the post will analyze possible confounding variables. Data for that analysis were statistics from &lt;a href="https://www.pro-football-reference.com/" target="_blank"&gt;pro football reference&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;A/B Testing&lt;/h2&gt;
&lt;ul&gt;
&lt;p&gt;&lt;li&gt;
&lt;strong&gt;Null Hypothesis:&lt;/strong&gt; &lt;em&gt;The rules implemented post 2017 and post 2018 had no effect on the number of concussions in a regular season. Any differences are due to random chance&lt;/em&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Alternative Hypothesis:&lt;/strong&gt; &lt;em&gt;The rules were statistically significant in decreasing the number of concussions in the regular season&lt;/em&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Statistic:&lt;/strong&gt; &lt;em&gt;The statistic we'll be measuring is the average number of concussions before the rule changes minus the average number of concussions after the rule changes&lt;/em&gt;
&lt;/li&gt;&lt;/p&gt;
&lt;/ul&gt;
&lt;p&gt;I chose to carry out the A/B Test with a permutation tests. For those not familiar, we set up the null and alternative as above. Under the null, we assume that no matter the rule changes in any year, it doesn't effect the number of concussions, so we can permute the concussion numbers and associate them with different years, then compute the statistic. We repeat this 10,000 times and then we have an accurate distribution of the statistics look like under the null hypothesis. Then we can plot the actual statistic. If it's far off from the distribution created, then we can say that the null hypothesis is very unlikely and we'll sway toward the alternative.&lt;/p&gt;
&lt;p&gt;Since targeting was one of the first major rules to address concussions, we'll start there. The control group are the concussion values for the years 2012 to 2016, and the treatment is 2017 to 2019 (the treatment being rules to help mitigate concussions). Here's the resulting permutation test visualization.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img not found" src="http://localhost:8080/assets/concussions_assets/2017_ab_test.jpg"&gt;
&lt;figcaption&gt;
Permutation test for concussions pre and post 2017 (2017 included in post). The blue line is the observed statistic
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;The blue line is the observed statistic found with the real NFl data. The graph clearly shows that reality is quite in line with the null hypothesis distribution created. In addition it has a &lt;em&gt;p-value of 0.39&lt;/em&gt;. It's clear that we fail to reject the null hypothesis here.&lt;/p&gt;
&lt;p&gt;The NFL stated the concussion rule changes a great success after a pretty steep decrease in concussions in 2018, so I decided to see if the new rules they implemented for the 2018 season had any significant effect. Here's the permutation test visualization.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img not found" src="http://localhost:8080/assets/concussions_assets/2018_ab_test.jpg"&gt;
&lt;figcaption&gt;
Permutation test for concussions pre and post 2018 (2018 included in post). The blue line is the observed statistic
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;Again, the blue line is the observed statistic. And while this fairs much better than the previous test, a &lt;em&gt;p-value of 0.145&lt;/em&gt; doesn't meet the 90% or 95% confidence threshold. Again, we fail to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;This quick analysis shows that on the surface level, the change in concussions over recent years have been due to random chance. However, there is one caveate that we haven't given the NFL enough credit for.&lt;/p&gt;
&lt;h2&gt;Data Augmentation&lt;/h2&gt;
&lt;p&gt;&lt;img alt="img not found" src="http://localhost:8080/assets/concussions_assets/time_series.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Above is the data we've been working with plotted as a time series. Notice the large dip in 2014. 2014 actually had by far, the largest number of unique players designated as "out" (injured and did not play for at least a game) of all the years analyzed at 605 players. The next closest year comes in at 470. It also had about the same number of kickoffs and plays as the other years. This huge drop in concussions is unprecedented. This is likely because of rule changes that resulted in players needing time to learn them, effecting their tackling. It could also be because player's were just more cautious (also because of rule changes or personal reasons), or teams were overly cautious. There could be more reasons or none of these reasons it's hard to tell, however, this dip in concussions is quite strange. Thus, I re-ran the A/B tests, but I changed the 2014 value to the mean of the concussions from 2012 to 2017 (the control and including 2014). This is a large change that doesn't necessarily reflect reality, but I think it's fun to see how this changes the statistical tests. I only ran the statistical test where the treatment were the years 2018 and 2019. It had a &lt;em&gt;p-value of 0.035&lt;/em&gt; and its visualization is below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img not found" src="http://localhost:8080/assets/concussions_assets/2018_ab_test_augmented.jpg"&gt;&lt;/p&gt;
&lt;p&gt;So now it seems like the NFL rules had a big change, only if 2014 was a more normal year. But this comes with it's own challenges. Before I brushed over some of the assumptions A/B tests follow. They require that the only difference between the control and treatment groups is the treatment itself. In other words, no confounding variables. This wasn't necessary to state before because we didn't find anything statistically significant even with possible confounding variables present.&lt;/p&gt;
&lt;p&gt;3 notable confounding variables come to mind. The number of unique players who were injured, number of plays, and number of kickoffs over the course of the regular season.&lt;/p&gt;
&lt;h2&gt;Multivariate Regression&lt;/h2&gt;
&lt;p&gt;In multivariate regression, we can include the confounding variables and see if the variable we're analyzing really is a strong predictor of the number of concussions. Without the data augmentation, we find that no matter the combination of indicator variables for years (pre-exclusive and post-inclusive 2018) and confounding variables, we still don't find that the rules put in place are statisticallly significant in reducing the number of concussions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img not found" src="http://localhost:8080/assets/concussions_assets/lm_res.png"&gt;
&lt;figcaption&gt;
Result of R's Linear Model summary function on concussions data with the year indicator variable along with confounding variables. unique_outs: Unique players classified as "out" and did not play in a game because of injury, plays: number of plays, scores: number of scores (TDs and kickoffs) meant to resemble the number of kickoffs. All variables are regular season stats from pro-football-reference [see Data section above].
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;year_indicator&lt;/code&gt; variable is what we're most interested in. It has a large standard error, so it's hard to say whether the estimate for this linear model is as estimated or should be 0. So in these terms, the NFL rules did not have much of an effect (if any at all) on the number of concussions in the regular season. However, how does it perform on the data with the outlier removed?&lt;/p&gt;
&lt;p&gt;As expected, it does much better. Combined with some confounding variables, such as only the unique number of players classified as "out" once in a season, it's a statistically significant predictor. However, when faced with mutliple confounding variables, it continues to miss the mark (i.e. it's hard to differentiate the estimate with 0 like before). So, depending on the confounding variables in reality, we can't say that the rules have significantly affected the decrease in concussions considering the data we have.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img not found" src="http://localhost:8080/assets/concussions_assets/lm_res_aug.png"&gt;
&lt;figcaption&gt;
Result of R's Linear Model summary function on augmented concussions data with the year indicator variable along with confounding variables. unique_outs: Unique players classified as "out" and did not play in a game because of injury, plays: number of plays, scores: number of scores (TDs and kickoffs) meant to resemble the number of kickoffs. All variables are regular season stats from pro-football-reference [see Data section above].
&lt;/figcaption&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The short answer to this question is no, the NFL's rules haven't done enough to say that they are the reason concussions were down in recent years. There is an argument to be made for the NFL that the data we have is very small and it's hard to come to statistical conclusions with this little data. However, when dealing with something as big as CTE and protecting the players that make the league money, the NFL should be doing something with extreme noticeable impact to help them. I hope to see a larger drop in concussions soon. I hope this post is shown to be wrong in the near future.&lt;/p&gt;</content><category term="Data Science"></category><category term="Causality Tests"></category><category term="Permutation Testing"></category><category term="Multivariate Regression"></category></entry><entry><title>Bay Area Twitter Sentiment During a Pandemic</title><link href="http://localhost:8080/ba-sentiment.html" rel="alternate"></link><published>2020-05-06T15:30:00-07:00</published><updated>2020-05-06T15:30:00-07:00</updated><author><name>Cameron Malloy</name></author><id>tag:localhost,2020-05-06:/ba-sentiment.html</id><summary type="html">&lt;p&gt;What is the overall sentiment of the bay area? What is the distribution of this sentiment? What are people happy or sad about in a given area? The San Francisco Bay Area is home to a very diverse population. There are many different people from different racial profiles.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;What is the overall sentiment of the bay area? What is the distribution of this sentiment? What are people happy or sad about in a given area?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The San Francisco Bay Area is home to a very diverse population. There are many different people from different racial profiles.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;figure markdown="span" id="ba_race_demos"&gt;
&lt;img alt="img not found" src="http://localhost:8080/assets/ba_sentiment_assets/ba_race_demos.jpg"&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;figcaption markdown="span"&gt;
Data taken from &lt;a href="http://www.bayareacensus.ca.gov/bayarea.htm" target="_blank"&gt;the bay area census&lt;/a&gt;.
Original data is from the &lt;em&gt;2010 United States Census¬†Summary File 1&lt;/em&gt;. United States Census Bureau
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;What's more interesting is that individuals of the same race tend to live around each other. The reasons behind this could be due to a multitude of reasons. One may want to be near social connections. Some others factors may be housing, employment, and police discrimination that separates races.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Race and Ethnicity 2010 - San Francisco Bay Area&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;figure markdown="span" id="ba_race_map"&gt;
&lt;img alt="img not found" src="http://localhost:8080/assets/ba_sentiment_assets/ba_race_map.png"&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;figcaption markdown="span"&gt;
Data from the &lt;em&gt;2010 United States Census¬†Summary File 1&lt;/em&gt;. United States Census Bureau. Each dot is 25 people: &lt;span id="red_color"&gt;White&lt;/span&gt;, &lt;span id="blue_color"&gt;African American&lt;/span&gt;, &lt;span id="green_color"&gt;Asian&lt;/span&gt;, &lt;span id="orange_color"&gt;Hispanic&lt;/span&gt;, or Other (yellow)
Map by: &lt;a href=https://www.flickr.com/photos/walkingsf/5560477152/ target="_blank"&gt;Eric Fischer&lt;/a&gt;
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, these racial clusters tend to define the cities they are in. Despite being in close proximity within each other, each city has its own distinct culture. This is what provoked the question in the beginning of this post. What if we figure out which cities are happier than others at certain points in time? With this classification, we can possibly find out what people are happy or concerned about, something we can't do without surveys and directly interacting with people.&lt;/p&gt;
&lt;p&gt;Even more interesting is that during the time of this analysis, COVID-19 has taken effect and the data gathered is during the time when the waves of extended shelter in place were announced. Therefore, models should have an easier time isolating COVID-19 responses.
However, if replicated, it may not lead to as interpretable results and may need further analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;See my &lt;a href="https://github.com/cameronmalloy/ba_sentiment" target="_blank"&gt;&lt;b&gt;github repository&lt;/b&gt;&lt;/a&gt; to repeat these results or conduct a similar analysis.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Data Gathering&lt;/h2&gt;
&lt;h3&gt;Training Dataset&lt;/h3&gt;
&lt;p&gt;I used the &lt;a href="http://help.sentiment140.com/for-students" target="_blank"&gt;Sentiment140&lt;/a&gt; dataset. It has a large dataset of tweets which uses distant supervision. They used emoticons to help label their tweets (labels were not created manually), read more from their &lt;a href="https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf" target="_blank"&gt;paper&lt;/a&gt;. This was the only dataset I found that was large enough to train a neural network and had justification in building the dataset. There were some kaggle datasets, but they didn't give any background on how labels were created, so I was concerned with the validity. One great thing about the Sentiment140 dataset is they impose some assumptions, but then each classified tweet is consistent and uses the same rules. That means human bias isn't imposed differently on every tweet, only imposed before classification.&lt;/p&gt;
&lt;h3&gt;Test Dataset&lt;/h3&gt;
&lt;p&gt;I combined two test datasets. The first one is provided by Sentiment140, the second is created by &lt;a href="https://github.com/zfz/twitter_corpus" target="_blank"&gt;Sanders Analytics&lt;/a&gt;. It's important to note that I didn't use Sentiment140's neutral tweets, and instead relied on Sanders Analytics neutral tweets. Since I trained the model on only positive and negative tweets, having a large number of neutral tweets to adjust thresholds to classify neutrality showed to be innacurrate. The thresholds would overlap (i.e. bigger than 0.4 should be positive and lower than 0.5 would be negative), but eliminating a large number of negative tweets resolved this issue. I looked over both datasets and I chose Sanders Analytics' neutral because the tweets were more diverse.&lt;/p&gt;
&lt;h3&gt;Application Dataset&lt;/h3&gt;
&lt;p&gt;The data gathered for this dataset uses &lt;a href="https://developer.twitter.com/en/docs/tweets/search/overview" target="_blank"&gt;Twitter's Standard Search API&lt;/a&gt;. It allows developers to access tweets within the past week along with user information. It's worthwhile to note that the API I was using (standard) has incomplete data fidelity.&lt;/p&gt;
&lt;p&gt;I had access to a large corpus of geotagged tweets, however, they didn't have the tweet's texts. Twitter's API couldn't find the tweets because it was so long ago. So, I decided to create my own dataset.&lt;/p&gt;
&lt;p&gt;I gathered all data within 40 miles of the center of the San Francisco Bay from Twitter's API. Twitter has a way of reverse geocoding tweets even if the user doesn't share their location. I gathered 700,000 tweets from this area that were tweeted between April 25th and May 2nd of 2020.&lt;/p&gt;
&lt;p&gt;Twitter allows users to set their location. They can set it to anything, it doesn't have to be their location at all. You could live in San Francisco but set your location to Kuala Lumpur. So I only took the tweets that had users whose location was a bay area city. The thought is that if they tweeted from within the bay area and their location is set to a bay area city, then there's a high likelihood that they live in that city. I ended up having about a quarter million tweets in my constructed dataset.&lt;/p&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;The goal is to build a classifier that takes in a tweet and outputs its sentiment, -1 for negative, 0 for neutral, and +1 for positive.&lt;/p&gt;
&lt;p&gt;I started with a baseline model of Naive Bayes, however that showed that it was slightly better than guessing. Logistic regression didn't converge. I didn't try an SVM because the training set had only positive and negative values, yet we also needed to compute the neutral tweets which would use thresholds on probabilities predicted by the classifier, which SVM's aren't necessarily great at. Then, I turned my attention toward neural networks, specifically LSTMs and GRUs. Due to financial and computational constraints, I was limited to Google Collaboratory for this part of the project, so BERT models didn't run well or else I would have fine tuned and compared those as well. Below are the accuracies for each model.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;figure markdown="span" id="ba_race_demos"&gt;
&lt;img alt="img not found" class="wp-image-104" src="http://localhost:8080/assets/ba_sentiment_assets/model_comparison.jpg"&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;figcaption markdown="span"&gt;
Numbers in parentheses are the number of units associated to the layer. Each LSTM and GRU layer are bidirectional. Each neural network had a dense layer with 64 output units with a ReLU activation followed by another dense layer with 1 output unit with a sigmoid activation. Also, each neural network had a global spatial dropout of 0.5. Due to computational and time constraints, each network was trained for 3 epochs. Each model is based on the threshold criteria of predicting neutral if the prediction is between 0.3 and 0.7 (not inclusive).
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;I chose to go forward with the GRU and convolutional network in bold in the table. This had less parameters than the GRU with 128 units, so this likely had a slightly lower variance. I trained this model for 10 epochs. After analyzing ROC curves to find the best thresholds the accuracy rose to slightly over 70% and gave the following confusion matrix.&lt;/p&gt;
&lt;figure id="ba_race_demos"&gt;
&lt;img alt="img not found" class="wp-image-105" src="http://localhost:8080/assets/ba_sentiment_assets/confusion_matrix_lstm_conv.png"&gt;&lt;/figure&gt;
&lt;p&gt;We can see that neutral tweets aren't the model's strength, but the classifier is quite good at predicting positive and negative tweets. This is likely due to the lack of neutral tweets. However, with a large enough sample size, the proportion of positive and negative tweets will likely approach the true result, and perturbations will be minimized. False positives and negatives are mostly due to sarcasm and tweets that require context.&lt;/p&gt;
&lt;h2&gt;Sentiment&lt;/h2&gt;
&lt;p&gt;Applying the model to the bay area tweets gives way to the following city sentiment map&lt;/p&gt;
&lt;figure&gt;
&lt;iframe width="100%" height="520" frameborder="0" src="https://cameronmalloy.carto.com/builder/436dbea1-daff-4fdd-b4ad-9dc1264360c3/embed" allowfullscreen webkitallowfullscreen mozallowfullscreen oallowfullscreen msallowfullscreen&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;figcaption&gt;
&lt;br /&gt;
Shapefile: &lt;a href="https://earthworks.stanford.edu/catalog/stanford-vj593xs7263"&gt;Stanford Earthworks&lt;/a&gt;&lt;/figcaption&gt;
&lt;p&gt;The map shows an average of all the tweets sentiment values from each city. Comparing just positive and negative tweets versus the overall average including neutral tweets yields similar values. The large number of tweets from each city discouraged me from normalizing any values based on tweet count and city population, since this average is likely to be close to the true average of the city. Almost all cities had large sample sizes (2,000 to 5,000) except a couple small ones like Yountville and St. Helena.&lt;/p&gt;
&lt;p&gt;It's important to note that almost every city was positive to an extent on average aside from a couple cities. The bay area seems to be a very positive place! I chose this color scheme, not so that pink and red represents negativity, but rather because those cities are more negative on average compared to the rest of the bay area.&lt;/p&gt;
&lt;p&gt;The stark contrast between the peninsula and East Bay is astonishing. The next section will be dedicated toward a way to extract what people are happy and sad about.&lt;/p&gt;
&lt;p&gt;Given the data was over one week during the extension of the COVID-19 shelter in place order, it would be interesting to see the sentiment for tweets relating to corona virus and the quarantine.&lt;/p&gt;
&lt;figure&gt;
&lt;iframe width="100%" height="520" frameborder="0" src="https://cameronmalloy.carto.com/builder/2bf5c2c2-b229-4321-bc7e-35704761c585/embed" allowfullscreen webkitallowfullscreen mozallowfullscreen oallowfullscreen msallowfullscreen&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;figcaption&gt;
&lt;br /&gt;
Shapefile: &lt;a href="https://data.sfgov.org/Geographic-Locations-and-Boundaries/Bay-Area-Counties/s9wg-vcph"&gt;San Francisco data.gov&lt;/a&gt;&lt;/figcaption&gt;
&lt;p&gt;Corona virus tweets accounted for 5% of the data and didn't affect the overall sentiments significantly. I had to plot this at the county level, so the sample sizes were large enough to give a proper estimate of the sentiments. It's important to note though that Napa county only had 7 tweets relating to COVID-19.&lt;/p&gt;
&lt;p&gt;Example corona virus tweets:&lt;/p&gt;
&lt;p&gt;Negative tweet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚úîÔ∏è Largest wildfires in modern history &lt;br /&gt;
‚úîÔ∏è Senate acquits of Donald Trump &lt;br /&gt;
‚úîÔ∏è Death of Kobe Bryant &lt;br /&gt;
‚úîÔ∏è Global Pandemic &lt;br /&gt;
‚úîÔ∏è Financial Crisis &lt;br /&gt;
‚úîÔ∏è "Murder Hornets" discovered in US &lt;br /&gt; &lt;br /&gt;
Whatchu cookin' up next, 2020? üôÉ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Neutral tweet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;My husband has been restaurant-phobic since this whole quarantine deal started up. I‚Äôve &lt;em&gt;finally&lt;/em&gt; convinced him to take a shot on some Chipotle takeout tonight. Nature is healing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This tweet may seem positive, however, from the model's point of view, the first sentence is negative and the second sentence is positive. The GRU puts more emphasis on complete sentences and likely separates the two out. So classifying this as a neutral tweet makes sense from the model's point of view. It also shows the faults of the model.&lt;/p&gt;
&lt;h2&gt;Clustering&lt;/h2&gt;
&lt;p&gt;The theory behind clustering these tweets stems from the thought that people who are tweeting about positive things are likely tweeting about the same things. Similarly, for negative tweets. So, we can separate the groups positive and negative tweets, and cluster them into groups and see if they have any similarities.&lt;/p&gt;
&lt;p&gt;If done correctly, this would give government and political officials a way of knowing what people are happy about and what people are unsatisified with in their city or governing area. Previously, the only way of consolidating such information would be through surveys which are expensive and could have a low turnout depending on the location.&lt;/p&gt;
&lt;h3&gt;Case Study: Berkeley, CA&lt;/h3&gt;
&lt;p&gt;The first clustering algorithm I used was K-Means, however, the data showed to be unfit for K-Means. Silhouette and elbow plots can be found in the repository under &lt;code&gt;LSTM.ipynb&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I switched to Agglomerative clustering. This type of clustering doesn't work well with large datasets, so I decided to take only Berkeley's tweets cluster those. Below is the dendrogram for Agglomerative Clustering using &lt;a href="https://en.wikipedia.org/wiki/Ward%27s_method"&gt;Ward's Method&lt;/a&gt; with Berkeley's positive and negative tweets.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img not found" src="http://localhost:8080/assets/ba_sentiment_assets/dendrogram_styled.jpg"&gt;
&lt;figcaption markdown="span"&gt;
I had to omit the x-axis values since there were so many samples, it looked like a thick black bar. With over 2,000 samples in each dendrogram, there was no way to fit it neatly on the x-axis.
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;Considering the relative success of Agglomerative Clustering versus K-Means, it's likely that the clusters could be intermingled within each other (for instance one cluster is a ring and another cluster is a circle within that ring), problems that K-Means can't solve well.&lt;/p&gt;
&lt;h4&gt;Positive Clustering&lt;/h4&gt;
&lt;p&gt;&lt;ol&gt;
&lt;li&gt;
    &lt;ul&gt;
    &lt;li&gt;Tweets about other people's happiness&lt;/li&gt;
    &lt;li&gt;wishing other people well&lt;/li&gt;
    &lt;li&gt;sharing other's pleasant discoveries&lt;/li&gt;
    &lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
    &lt;ul&gt;
    &lt;li&gt;Thanking people! Short and sweet tweets, typically of the form &lt;code&gt;@twitterhandle Thanks!&lt;/code&gt;&lt;/li&gt;
    &lt;li&gt;Happiness derived from individual experiences&lt;/li&gt;
    &lt;li&gt;Gratefulness&lt;/li&gt;
    &lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
    &lt;ul&gt;
    &lt;li&gt;Happy about receiving something new (job, earrings, etc.)&lt;/li&gt;
    &lt;li&gt;Non-mainstream news&lt;/li&gt;
    &lt;li&gt;Advertising (&lt;code&gt;check out our ...&lt;/code&gt;)&lt;/li&gt;
    &lt;li&gt;UC Berkeley affiliated accounts&lt;/li&gt;
    &lt;li&gt;Less individual, more about group happiness or directed toward groups of people&lt;/li&gt;
    &lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;
&lt;h4&gt;Negative Clustering&lt;/h4&gt;
&lt;p&gt;&lt;ol&gt;
&lt;li&gt;Media accounts, very little personal accounts, mostly about the COVID-19 situation&lt;/li&gt;
&lt;li&gt;Large-ish tweets (100-200 characters) about various negatives such as Donald Trump, Joe Biden, personal experiences like kids falling off skateboards, and work&lt;/li&gt;
&lt;li&gt;
    &lt;ul&gt;
    &lt;li&gt;Not much media, mostly personal accounts with 14% being about corona virus and quarantine&lt;/li&gt;
    &lt;li&gt;Dissatisfaction with the federal government&lt;/li&gt;
    &lt;li&gt;Dissatisfaction with Biden (recent sexual assault allegations). Far more tweets about this than the media, likely because Berkeley isn't a news/media giant.&lt;/li&gt;
    &lt;li&gt;Angry tone&lt;/li&gt;
    &lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Miscellaneous&lt;/li&gt;
&lt;li&gt;Very similar to cluster 2, except the size of the tweets is much smaller (around 50 characters). This is expected since the embeddings are padded to the same length, so shorter tweets would be in a smaller subspace.&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;
&lt;h3&gt;Case Study Results&lt;/h3&gt;
&lt;p&gt;This clustering was able to distinguish the corona virus tweets. Here's a glance at the types of tweets you get from that cluster:&lt;/p&gt;
&lt;ol&gt;
&lt;li markdown="1"&gt;

&lt;blockquote&gt;
&lt;p&gt;@[redacted] WE CAN'T POSSIBLY HAVE ANY FUN TILL AFTER THIS LOCKDOWN IS OVER WITH ALONG WITH A CURE FOR THE CORONA VIRUS ALONG WITH A VACCINE MADE THAT PEOPLE CAN TAKE MAKING IT WHERE WE CAN'T CATCH THE CORONA VIRUS!
&lt;/li&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;li&gt;&lt;/li&gt;
&lt;blockquote&gt;
&lt;p&gt;@[redacted] People who want to reopen the state at the cost of service workers and others lives
&lt;/li&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;li&gt;&lt;/li&gt;
&lt;blockquote&gt;
&lt;p&gt;There are legitimate concerns in the Democratic camp about what kind of steps other actors, including foreign actors and people with financial interests, will use to attack Joe Biden and other candidates. Assault accusations are probably not off the table on that front.
&lt;/li&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/ol&gt;
&lt;p&gt;The clusters aren't perfect, but the ability for it to extract a large number of tweets relating to the pandemic is promising. At a quick glance at the cluster, you can find a noticeable amount of related tweets to COVID-19. Once the pandemic dies down, the cluster will likely be lost, however, it could also pick up on other community dissatisfaction in the future.&lt;/p&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;We started with a dataset of tweets from the bay area and stripped it down to tweets that were likely to be from people who lived in certain cities within the bay.&lt;/p&gt;
&lt;p&gt;The LSTM + convolution network performed the best out of the machine learning algorithms attempted and was able to classify positive and negative tweets pretty well. Applying that to the dataset gave us which cities are happier on average compared to others.&lt;/p&gt;
&lt;p&gt;Lastly, we grouped the positive and negative tweets into clusters from tweets in Berkeley. We found interesting patterns in the clusters, but the most important was the cluster that extracted COVID-19 negative tweets, which points to a possibility of algorithmically detecting distress in cities.&lt;/p&gt;
&lt;h2&gt;Future Work&lt;/h2&gt;
&lt;p&gt;The training dataset doesn't include neutral tweets. It would be very beneficial if there was a way to include this, however, it seems almost unfeasible at the moment.&lt;/p&gt;
&lt;p&gt;I only attempted recurrent networks, but attention networks may prove to have far better results.&lt;/p&gt;
&lt;p&gt;K-Means clustering showed to be undesirable, however, agglomerative clustering had some promising results. It has the caveat of having very poor scalability. Also, the idea of clustering tweets is very difficult given the very large space the tweets lie in. Other clustering methods may show to be better, such as spectral clustering and DBSCAN (which also has the scalability disadvantage).&lt;/p&gt;
&lt;p&gt;Of course this analysis becomes way more interesting with geo-tagged tweets. The problem is the scarcity of them, sepecially in this dataset which hovered around 500 tweets throughout the Bay Area, thus, it's hard to make any conclusive results or visualizations. Obtaining a large corpus of this could lead to some interesting results.&lt;/p&gt;
&lt;h2&gt;Ethical concerns&lt;/h2&gt;
&lt;p&gt;Twitter data poses ethical concerns. While their terms of service states that tweets may be used by 3rd parties, it's difficult to determine if people in the Bay Area were ok with their data being used for this analysis, and ok for me personally to look directly at their tweets. I however, didn't match twitter handle with tweets aside from looking at the general structure of the json output from the api, but I would figure many people still wouldn't be ok with this. There's also the fact that users may delete their tweets, but those tweets will still be represented in this post. Although aggregated with many others, it's hard to tell.&lt;/p&gt;
&lt;p&gt;Twitter has a reverse geolocation feature which allows developers to grab tweets from a certain location even if users don't opt into geotagging their tweets. This allowed me to collect a lot of data, but it also allows 3rd parties to get a relative area where users tweeted, even if they don't want to give away this information. I'm sure Twitter handles this in their terms and services, however, I never knew of this feature before starting this project. Reverse geotagging should be made more transparent, because the fact 3rd parties can map out which cities users come from without opting into geotagging is concerning.&lt;/p&gt;
&lt;p&gt;Also, interpretation of this analysis can lead to misleading results. For instance, this analysis relies solely on Twitter data, which may not be representative of the population you're analyzing. The bay area has a thriving Twitter following, so large issues affecting many people will likely be relayed through Twitter eventually. However, this may not be the case in cities with small Twitter followings (and even so in the bay area like in Napa County). I would suggest further testing to see for how many people are worried about some problem, 1 person will eventually tweet about it, and capturing this number will give an essence as to what kind of clusters and problems you can detect.&lt;/p&gt;
&lt;h2&gt;Addendum&lt;/h2&gt;
&lt;p&gt;I followed up and did the same sentiment classification a month later (May 25th to June 2nd 2020), when tensions were very high after the death of &lt;a href="https://www.nytimes.com/2020/05/31/us/george-floyd-investigation.html" target="_blank"&gt;George Floyd&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;iframe width="100%" height="520" frameborder="0" src="https://cameronmalloy.carto.com/builder/ee1c6026-4846-4ae6-b699-f2bdc1cca586/embed" allowfullscreen webkitallowfullscreen mozallowfullscreen oallowfullscreen msallowfullscreen&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;figcaption&gt;
&lt;br /&gt;
Shapefile: &lt;a href="https://earthworks.stanford.edu/catalog/stanford-vj593xs7263"&gt;Stanford Earthworks&lt;/a&gt;&lt;/figcaption&gt;
&lt;p&gt;Comparing this map to the previous map, the sharp difference between the peninsula and east bay remains. However, the bay area, especially those closest to metropolitan areas, are largely more negative on average. Regions around Oakland saw the sharpest drop, likely because of the racial demographics of the region and having a prominent Black population. Even regions around the pensinula like the San Mateo region saw a large drop similar to the East Bay as well. In fact, the drop around Oakland is statistically significant, as shown with the permutation test visualization below. The permutation test has a null hypothesis that the difference between the May and June sentiments are the same and an alternative that June sentiments are much smaller. The test results in a p-value of 0.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;figure markdown="span" id="ba_race_map"&gt;
&lt;img alt="img not found" src="http://localhost:8080/assets/ba_sentiment_assets/oakland_permutation_test.jpg"&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;figcaption markdown="span"&gt;
Permutation code can be found on the &lt;a href="https://github.com/cameronmalloy/ba_sentiment/blob/master/data/sf_tweets/oakland_permutation_test_notebook.ipynb" target="_blank"&gt;github repo&lt;/a&gt;
&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;The numbers are also skewed positive in this instance since there are a number of tweets that confused the classifier. For instance, tweets such as&lt;/p&gt;
&lt;ol&gt;
&lt;li markdown="1"&gt;

&lt;blockquote&gt;
&lt;p&gt;Oh my god [Image of violent protests]
&lt;/li&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;li&gt;&lt;/li&gt;
&lt;blockquote&gt;
&lt;p&gt;Surely there is a hero out there somewhere?
&lt;/li&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/ol&gt;
&lt;p&gt;There are a large number of tweets trying to spread love and show support for the movement in the bay area. This, along with the stock market trending up consistently since the corona virus outbreak and Census 2020 tweets make for a lot of things to be positive about. I'll leave the cluster analysis for the reader, however, it's relatively similar to what was stated above, except Covid tweets are replaced with the Black Lives Matter movement and the protests.&lt;/p&gt;</content><category term="Data Science"></category><category term="NLP"></category><category term="Deep Learning"></category><category term="Clustering"></category></entry></feed>